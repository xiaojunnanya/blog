---
id: aboutaiconcept
slug: /aboutaiconcept
title: 相关概念
date: 2002-09-26
authors: 酒辞.
tags: [AI]
keywords: [AI]
---



## 模型参数

- 定义：神经网络可学习的参数数量，决定"智商"的上限
- 单位：B（十亿），案例：DeepSeek R1满血版=671B参数
- 影响：参数越多，理解力/推理力越强，但显存需求高



## 上下文长度

- 定义：单次处理最大输入token数，决定“能读多长文本”
- 典型值：2K~128K，DeepSeek-R1支持128K（～6万汉字)
- 类比：人类“工作记忆”vs“长期记忆”，上下文不足会“断片”

> 上下文不够?模型会‘断片’，前文忘后文



## 思维链（COT）

- 展示推理步骤,提升可解释性。例如，DeepSeek通过开放内部推理路径，让用户理解AI是如何得出结论的。



## 最大输出长度

- 单次生成内容的上限，通常以Token为单位。如R1支持8Ktokens，对于长文本，需分多轮生成。



## 量化

- 定义:FP32浮点参数压缩为INT8/INT4整数，牺牲精度换效率
- 压缩比:体积缩小4-8倍，速度提升2-4倍，能耗降至1/10
- 代价:精度损失5%~15%，依赖Tensor Core/H100等硬件

> 类似于，把4k的图片压缩到1080p，得到的是更小的体积，更快的加载速度，失去的是画质



## 模型蒸馏

- 定义：大模型(Teacher）通过知识迁移，将其能力传授给更轻量的小模型(Student)。
- 案例：以DeepSeek-R1作为教师模型，指导Qwen-7B等学生模型，使其在特定任务上表现优异。
- 优缺点
  - 优点：模型更小、运行更快、更节省资源、易于部署。
  - 缺点：通常会损失一部分原始大模型的创造性和泛化能力。



## Token

- 定义：处理文本的最小单位，可以是一个词、一个字或一个标点符号，是Al服务计费的基础。
- 换算：1个英文字符约等于0.3个token，1个中文字符约等于0.6个token。
- 注意：不同模型的分词策略（Tokenizer)不同，实际token数量需以API返回结果为准。



## MOE架构

- 全称：Mixture of Experts(混合专家模型)
- 原理：多个“专家子模型”+门控调度器，按任务激活对应的专家
- 优势：计算高效，如DeepSeek-V3总参大，激活仅37B



## RAG 

- 全称：Retrieval-Augmented Generation(检索增强生成)
- 流程：提问→检索外部知识库→生成答案
- 优势：解决知识滞后,提升事实准确性,可结合多模态检索

> 不让AI瞎编——先查资料再说话



## 强化学习

- 对比：监督学习(模仿)vs 强化学习(试错+奖励)
- 机制：答对→奖励→强化路径;答错→惩罚→调整策略
- 优势：泛化能力强，适合数学/编程等复杂推理

> “像孩子学走路——摔倒了,才知道怎么走稳。”



## Agent 智能体

- 定义：感知环境、自主决策、执行任务的Al实体
- 等级演进：聊天机器人→推理者→智能体→创新者→组织者
- 2025趋势：智能体元年，案例:AutoGPT、BabyAGl

> “从‘动口’到‘动手’——AI真正开始干活了。”



## MCP
